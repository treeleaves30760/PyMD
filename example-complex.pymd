import pymd
# # Complex Code Example PyMD
#
# ## Import the Module
#
# ```
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
print('### Finish the import')
# ```
#
# ## Basic Setting
#
# ```
# Load the specific Gemma model
model_name = "google/gemma-3-1b-it"

# Determine the best device and avoid MPS issues
device = (
    "cuda" if torch.cuda.is_available()
    else "cpu"  # Use CPU instead of MPS to avoid allocation issues
)

# Use appropriate dtype for the device
dtype = torch.float32 if device == "cpu" else torch.float16

print(f"Using device: {device}, dtype: {dtype}")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load model with explicit device placement (avoid device_map="auto" for MPS)
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    torch_dtype=dtype,
    low_cpu_mem_usage=True,
)

# Manually move to device to avoid MPS issues
model = model.to(device)

print(f'### Model loaded successfully on {device} with dtype {dtype}')
# ```
#
# ## Start generating
#
# ```
try:
    # Ask about TCP/IP
    prompt = "Explain TCP/IP in simple terms. What is it and how does it work?"
    
    print(f"Generating response for: {prompt[:50]}...")
    
    # Tokenize and move to correct device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    # Generate with conservative parameters for stability
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1,
        )
    
    print("Generation completed successfully!")
    
except RuntimeError as e:
    if "MPS" in str(e) or "device" in str(e).lower():
        print("Device error encountered, trying fallback to CPU...")
        # Fallback: move everything to CPU
        model_cpu = model.to("cpu")
        inputs_cpu = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model_cpu.generate(
                **inputs_cpu,
                max_new_tokens=500,
                do_sample=False,  # Greedy for reliability
                pad_token_id=tokenizer.eos_token_id,
            )
        print("CPU fallback generation completed!")
    else:
        raise e
# ```
#
# ## Output the response
#
# ```
# Display the response
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Clean up the response by removing the original prompt
if response.startswith(prompt):
    clean_response = response[len(prompt):].strip()
else:
    clean_response = response.strip()

print("### Gemma-3 Response about TCP/IP:")
print("=" * 50)
print(clean_response)
print("=" * 50)
print("âœ… **Generation completed successful!**")
# ```
#
# ## This is a LLM text generation example with PyMD
#
# This example demonstrates:
# - **Complex model loading and device management**
# - **Error handling with fallback strategies**  
# - **AI text generation integration**
# - **Markdown output formatting from code execution**
#
# The file can be run directly as Python: `python example-complex.pymd`